---
title: "Analysis Pilot CommunicationBlocks Experiment"
output:
  html_document:
    df_print: paged
---

```{r, packages, message=F}
library(here)
library(tidyverse)
library(dplyr)
library(brms)
library(tidybayes)
#options(dplyr.summarise.inform = FALSE)

source(here("R", "utils.R"))
theme_set(theme_bw(base_size=12) + theme(legend.position = "top"))
```

```{r, message=FALSE, warning=FALSE}
fn <- "results_67_communicationBlocks_BG_pilot.csv"
data <- read_csv(here("results", fn)) %>% as_tibble() %>%
  group_by(submission_id)
```

## Meta information about participants

```{r, meta, echo=FALSE}
data.meta <- data %>% distinct_at(vars(c(submission_id)), .keep_all = T) %>%
  mutate(education=as_factor(education), gender=as_factor(gender))

data.meta$gender %>% summary()
data.meta$education %>% summary()
print("age:")
data.meta$age %>% summary()
print("timeSpent:")
data.meta$timeSpent %>% summary()
```

## Reaction times

```{r, timespent, message=FALSE}
data.rt <- data %>% 
  select(expected, RT, id, question, trial_name, type, timeSpent, trial_number, 
         response)
data.rt %>%
  ggplot(aes(x=id, y=RT)) + 
  geom_jitter(aes(color=question), width=0.2, height=0) +
  geom_boxplot(outlier.shape=NA) +
  theme(axis.text.x=element_text(angle=90), legend.position = "right") +
  facet_wrap(~type, scales="free_x")
```

### Reaction Times per trial-type

```{r, message=FALSE, results=FALSE, echo=FALSE, warning=FALSE}
data.rt %>% 
  group_by(submission_id) %>%
  mutate(timeAll = timeSpent * 60,
         timeExperiment = (sum(RT) / 1000),
         timeInstructions = timeSpent - timeExperiment,
         timeTraining = (sum(RT[type=="training"]) / 1000),
         timeTesting = timeExperiment - timeTraining
         ) %>% 
  summarise(timeAll, timeTraining, timeTesting) %>% 
  unique()
# first digits before point are in seconds 
```

```{r}
train_times = configure(c("train_times")) %>% as_tibble() %>% 
  pivot_longer(cols=everything(), names_to="id", values_to="min")
data.rt.train = data.rt %>% 
  mutate(correct = response == expected) %>% 
  group_by(submission_id, id, trial_number, correct) %>%
  filter(type == "training") %>% 
  summarize(time_sec=sum(RT)/1000) %>% 
  arrange(submission_id, trial_number)

data.rt.train %>% mutate(submission_id = as.factor(submission_id)) %>% 
  ggplot(aes(x=id, y=time_sec)) + 
  geom_boxplot(outlier.shape=NA) + geom_jitter() +
  geom_point(data=train_times, aes(x=id, y=min), size=1.5, color='red')
```

Red data points are my estimates of the minimal time needed to watch the entire
animation to see which blocks fall. 

```{r, include=FALSE, echo=FALSE}
left_join(data.rt.train, train_times, by="id") %>% 
  filter(time_sec < min)
```

Training time:
- Fast run took me **53 seconds** while waiting for the animations,
- Fast run without waiting for the animations took **32 seconds**
- Thorough (slow) run took **99 seconds**
- Thus, anyone **below ~50 seconds** time might not have fully watched the animations (or understood the training)

--> Only one person (ID 2234) took below 50 secs (49s) but it is so marginal, I think training times are fine.


Test Time:
- Fast run took me **185 seconds** while still reading all text
- Very fast run took **117 seconds** only browsing over the questions quickly and sometimes clicking sloppily
- Thorough (slow) run took **244 seconds**
- Thus, anything **below ~150 seconds** was not done thorough enough and the person might not have read the questions properly

--> Two people took below 130 seconds (ID 2243 = 148s and ID 2237 = 114s). It might be possible to be as fast as 114 seconds while still fulfilling the task but only if you are highly concentrated. 

### Training time vs. nb correct answers

```{r, relation-train-rt, warning=FALSE, message=FALSE}
train.time_total = data.rt %>% group_by(submission_id, type) %>% 
  mutate(correct = response == expected) %>% 
  summarize(time_sec = sum(RT)/1000, n_correct = sum(correct)) %>% 
  filter(type == "training")

train.time_total %>% 
  ggplot() + geom_point(aes(x=n_correct, y= time_sec)) +
  geom_hline(aes(yintercept=train.time_total$time_sec %>% mean()), color='red') +
  geom_hline(aes(yintercept=train.time_total$time_sec %>% median()), color='blue')


```

### Reaction times per train trial for correct/wrong trials

```{r}
data.rt.train %>%
  ggplot(aes(y=id, x=time_sec, color=correct)) +
  geom_jitter(height=0.2, width=0) +
  geom_boxplot(outlier.shape=NA) +
  labs(x="Reaction time in seconds", y = "training id") 
```


## General comments all participants

```{r}
data %>% select(comments) %>%  filter(!is.na(comments) & comments!="") %>% 
  distinct()
```

## Training Trials

```{r}
data.train = data %>% filter(type=="training") %>%
  mutate(correct=case_when(expected == response ~ T, T ~ F))

# ratio correct per trial number (different order per participant)
train.correct.nb = data.train %>% group_by(trial_number) %>% 
  summarize(ratio_correct = mean(correct))
train.correct.nb %>% 
  ggplot(aes(x=trial_number, y=ratio_correct)) + 
  geom_bar(stat="identity") + 
  scale_x_continuous(breaks=seq(1, 7))

# ratio correct per trial id
train.correct.id = data.train %>% group_by(id) %>% 
  summarize(ratio_correct = mean(correct))
train.correct.id %>% 
  ggplot(aes(x=id, y=ratio_correct)) + 
  geom_bar(stat="identity") +
  theme(axis.text.x=element_text(angle=90))
```

## Example Test-Trial

```{r, warning=FALSE}
data.example_test = data %>% filter(type=="test-example") %>%
  group_by(submission_id) %>% 
  dplyr::select(expected, selected_pic) %>% 
  transmute(correct = expected == selected_pic)

rate_correct_example = data.example_test %>%
  summarize(ratio_correct = sum(correct) / n()) %>% 
  pull(ratio_correct)

ids.wrong = data.example_test %>% filter(!correct) %>% pull(submission_id) %>% 
  unique()
ids.wrong
```


## Test trials

Look at control test trials

```{r, message=FALSE}
data.control = data %>% filter(startsWith(type,"control")) %>% 
  mutate(correct = expected == selected_pic) %>% 
  group_by(id, type) %>%
  mutate(n=n(), n_correct = sum(correct), ratio_correct = n_correct/n) %>% 
  dplyr::select(c(id, type, n, n_correct, ratio_correct)) %>%
  distinct_at(vars(c(id, type)), .keep_all = T)
    
data.control %>% 
  ggplot(aes(x=ratio_correct, y=id)) + 
  geom_bar(aes(fill=type), stat="identity") +
  labs(x="ratio correct", title="control test trials", y="")
```

Clean data

```{r}
n_all = data$submission_id %>% unique() %>% length
data.cleaned = clean_data(data)
#from training trials:
# data.cleaned <- data.cleaned %>% filter(!submission_id %in% c(2233, 2237))
n_kept = data.cleaned$submission_id %>% unique() %>% length

ratio_kept = round(n_kept/n_all, 2)
ratio_kept
```

Data from `r ratio_kept` of all participants is retained.

## Plot critical trials

```{r, data-critical, echo=FALSE}
data.critical = data.cleaned %>% 
  filter(type=="critical") %>% 
  select(submission_id, id, type, question, response, expected, id1, id2,
         selected_pic, RT) %>% 
  mutate(expectation = case_when(expected == "none" ~ FALSE,
                                 TRUE ~ TRUE)) %>%
  group_by(id1, id2)

data.critical$pic_combi = data.critical %>% group_indices()
data.critical <- data.critical %>% 
  mutate(pic_combi=paste(id1, id2, sep=" & ")) %>% 
  group_by(pic_combi, question) %>% mutate(n=n()) 
```

With 95%-bootstrap confidence intervals

```{r, boostrap-samples, echo = FALSE}
N = 1000
n = data.critical$submission_id %>% unique() %>% length()
df.all = data.critical %>% dplyr::select(submission_id, pic_combi, question, response)

bootstrap_samples = group_map(df.all, function(df, df.group){
  bootstrap = map(seq(1, N), function(i){
    tibble(rate_exhaustive =
             sum(sample(df$response, n, replace=T) == "exhaustive")/n)
  }) %>% bind_rows() %>% 
    add_column(pic_combi = df.group$pic_combi, question = df.group$question) %>% 
    arrange(rate_exhaustive)
  
  CI.bounds = bootstrap_bounds(bootstrap)
  ci.low = bootstrap[CI.bounds[["low"]], ]$rate_exhaustive
  ci.up = bootstrap[CI.bounds[["up"]], ]$rate_exhaustive
  bootstrap %>% add_column(ci.low=ci.low, ci.up=ci.up)
}) %>% bind_rows() %>% group_by(pic_combi, question)

bootstraps.CI.question_pic = bootstrap_samples %>%
  dplyr::select(-rate_exhaustive) %>% 
  distinct()

# CIs bounds across pic_combis (same for each question)
CI.bounds.question = bootstrap_samples %>% 
  filter(question=="cons") %>% 
  bootstrap_bounds()

df = bootstrap_samples %>% dplyr::select(-ci.low, -ci.up) %>%
  group_by(question) %>% arrange(rate_exhaustive)
bootstraps.CI.question = group_map(df, function(samples, grp) {
  ci.low = samples[CI.bounds.question[["low"]], ]$rate_exhaustive
  ci.up = samples[CI.bounds.question[["up"]], ]$rate_exhaustive
  samples %>% add_column(ci.low=ci.low, ci.up=ci.up, question=grp$question)
}) %>% bind_rows() %>%
  dplyr::select(-rate_exhaustive, -pic_combi) %>% distinct()
```

```{r, critical-data-rates, echo = FALSE}
data.critical.rate = data.critical %>% group_by(question) %>% 
  summarize(rate_exhaustive = sum(response == "exhaustive") / n(),
            .groups = "drop_last") 

data.critical.rate = left_join(
  data.critical.rate,
  bootstraps.CI.question, 
  by="question"
)

data.critical.rate.pic_combi = data.critical %>% 
  group_by(question, pic_combi) %>% 
  summarize(rate_exhaustive = sum(response == "exhaustive") / n(),
            .groups="drop_last") 
data.critical.rate.pic_combi <- left_join(
  data.critical.rate.pic_combi,
  bootstraps.CI.question_pic,
  by = c("question", "pic_combi")
)
```

1. across all different instantiations, i.e. exhaustive/non-exhaustive 
picture combinations

```{r}
conditions = configure(c("conditions"))
data.critical %>%
  ggplot(aes(fill=response, y=question)) + 
  geom_bar(position=position_dodge(preserve = 'single')) +
  scale_y_discrete(breaks=names(conditions), labels = conditions) + 
  labs(y="Ann's question")

# same plot with exhaustive-picture selection ratios instead of absolute numbers
data.critical.rate %>%
  ggplot(aes(x=rate_exhaustive, y=question)) + 
  geom_bar(stat="identity") + 
  geom_errorbar(aes(xmin=ci.low, xmax=ci.up)) +
  geom_text(aes(label=round(rate_exhaustive, 2)), hjust=1, color='white') +
  scale_y_discrete(breaks=names(conditions), labels = conditions) + 
  labs(y="Ann's question", x="selection rate exhaustive picture")
```

2. split among different instantiations of critical trials 
(i.e., across different picture combinations)

```{r}
df = data.critical.rate.pic_combi %>% group_by(pic_combi) %>% 
   mutate(group_id = cur_group_id(), 
          group_id = chartr('1234', 'ABCD', group_id)) %>% 
  unite("pic_combi", "group_id", "pic_combi", sep=". ")

df %>% 
  ggplot(aes(x=rate_exhaustive, y=question)) + 
  geom_bar(stat="identity") + 
  geom_errorbar(aes(xmin=ci.low, xmax=ci.up)) +
  geom_vline(aes(xintercept= 0.5), color='green', linetype='dashed') +
  geom_text(aes(label=round(rate_exhaustive, 2)), hjust=1, color='white') +
  facet_wrap(~pic_combi)
```

## Observations 

1. *if1_un & if2_unu* (B) and *if2_unn & if2_unu* (D): preference for exhaustive situation
2. *if2_unn & if1_uu* (C): preference for non-exhaustive situation
3. *if1_un & if1_uu* (A): avg selection rate < 0.5, but 95% bootstrap CI-interval contains 0.5, i.e.no particular strong preference for non-exhaustive situation here
4. no difference observed between *if_ant* and cons conditions for any picture combi except maybe for *if1_un & if2_unu* (B).

## Possible reasons

1. For the *will_cons* condition, there might be a tendency towards the situation where the consequent-block is more likely to fall per se (came to my mind when watching someone doing the experiment). This only applies to *if1_un & if1_uu* (A) and *if2_unn & if1_uu* (C) conditions. In the other two conditions the consequent-block is positioned identical, namely completely on the platform.  

2. The stronger tendency towards the non-exhaustive situation in condition *if2_unn & if1_uu* (C) might be due to the fact that in the non-exhaustive situation only the green and the blue block are shown, which are the only blocks Bob talks about whereas for the condition *if1_un & if1_uu* (A), neither situation shows a yellow block.

3. Generally, it might be the case that participants tend to prefer the situation that involves *less* uncertainty. I'd say that these correspond to the following: 
  
  - A. *if1_un & if1_uu*: if1_un (non-exhaustive)
  - B. *if1_un & if2_unu*: if1_un (exhaustive)
  - C. *if2_unn & if1_uu*: ? (if1_uu because no yellow block? or if2_unn, because position might be clearer?)
  - D. *if2_unn & if2_unu*: if2_unn (exhaustive)


## Bayesian regression model

```{r, results='hide'}
df = data.critical %>% 
  mutate(y=case_when(response == "exhaustive" ~ 1, T ~ 0)) %>%
  group_by(pic_combi) %>% 
   mutate(group_id = cur_group_id(), 
          pics = chartr('1234', 'ABCD', group_id)) %>% 
  group_by(pics, question) %>%
  mutate(pics = as.factor(pics), 
         question = factor(question, levels = c("neutral", "if_ant", "cons"))) %>%
  rename(submissionID = submission_id)

fit <-
  brm(data = df,
      family = bernoulli,
      y ~ 1 + pics * question + (1 + pics + question|submissionID),
      seed = 1)
fixef(fit)
pp_check(fit)
mcmc_plot(fit)
```

```{r, regression-hypotheses}
post_samples = tidy_draws(fit)
hypothesis(fit, "questionif_ant + picsB:questionif_ant < questioncons + picsB:questioncons", class = "b")

```

**General thoughts**:

  - Maybe show two pictures only after participants have read Ann's response? 
  This would hopefully make clear that Ann's question is not necessarily the 
  best question to distinguish both pictures. Rephrase, e.g. Ann is interested in 
  what happens if the antecedent-block falls. 
  
  - Maybe also rephrase Bob's response? E.g. 'In that case, ... will fall' or put
  consequent first, e.g. 'the consequent-block will fall if the ...'. 

  - Maybe we would need to make Bob say in a control-trial something like 'If 
  blue or yellow, green will fall'. This would emphasize the presence of the 
  yellow block in general Iâ€™d think.We may also have such a trial directly
  before *if2_unn vs. if2_unu* (yellow block in both situations) one to further
  direct attention towards the yellow block.

### Look at RTs for critical test-trials

```{r}
data.critical %>%
  ggplot(aes(x=RT, y=question)) + 
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha=0.5) +
  facet_wrap(~pic_combi, scales="free")
```

